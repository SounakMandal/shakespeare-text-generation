{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file nameshakespeare_sonnets\n",
      "Total character count :  94275\n"
     ]
    }
   ],
   "source": [
    "filename = input('Enter file name')\n",
    "with open(filename+'.txt') as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(\"Total character count : \", len(shakespeare_text))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count :  OrderedDict([('t', 7216), ('h', 5070), ('e', 9249), (' ', 15679), ('s', 4991), ('o', 5707), ('n', 4521), ('\\n', 2468), ('b', 1230), ('y', 1986), ('w', 1898), ('i', 4677), ('l', 3093), ('a', 4941), ('m', 2095), ('k', 553), ('p', 1011), ('r', 4183), ('f', 1663), ('c', 1342), ('u', 2320), ('d', 2763), (',', 1880), (\"'\", 386), ('g', 1358), ('v', 925), (':', 160), ('-', 83), ('.', 396), ('z', 20), (';', 31), ('x', 60), ('?', 92), ('q', 51), ('j', 68), ('(', 43), (')', 43), ('!', 23)]) \n",
      "\n",
      "Document size :  1 \n",
      "\n",
      "Index of tokens {' ': 1, 'e': 2, 't': 3, 'o': 4, 'h': 5, 's': 6, 'a': 7, 'i': 8, 'n': 9, 'r': 10, 'l': 11, 'd': 12, '\\n': 13, 'u': 14, 'm': 15, 'y': 16, 'w': 17, ',': 18, 'f': 19, 'g': 20, 'c': 21, 'b': 22, 'p': 23, 'v': 24, 'k': 25, '.': 26, \"'\": 27, ':': 28, '?': 29, '-': 30, 'j': 31, 'x': 32, 'q': 33, '(': 34, ')': 35, ';': 36, '!': 37, 'z': 38} \n",
      "\n",
      "Unique characters :  38 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count = tokenizer.word_counts\n",
    "document_count = tokenizer.document_count\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Token count : \", word_count, \"\\n\")\n",
    "print(\"Document size : \", document_count, \"\\n\")\n",
    "print(\"Index of tokens\", word_index, \"\\n\")\n",
    "print(\"Unique characters : \", len(word_count), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94275,)\n"
     ]
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = encoded.shape[0]\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\r\n",
    "n_steps = 100\r\n",
    "window_length = n_steps+1\r\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\r\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000021665F87700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000021665F87700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = len(word_count)), Y_batch)\n",
    ")\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\logs_shakespeare_sonnets\\1624468336\\date_2021_06_23-time_22_42_16\n",
      ".\\checkpoints_shakespeare_sonnets\\1624468336\\1624468336.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "model_name = int(time.time())\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, f\"logs_{filename}\")\n",
    "run_id = time.strftime(\"date_%Y_%m_%d-time_%H_%M_%S\")\n",
    "log_dir = os.path.join(root_logdir, str(model_name)+\"\\\\\"+run_id)\n",
    "print(log_dir)\n",
    "\n",
    "root_filepath = os.path.join(os.curdir, f\"checkpoints_{filename}\")\n",
    "filepath = os.path.join(root_filepath, str(model_name)+f\"\\\\{model_name}.h5\")\n",
    "os.mkdir(os.path.join(root_filepath, str(model_name)))\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath, monitor='loss', verbose=1, save_best_only=False\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1,\n",
    "        update_freq=10, write_graph=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, None, 128)         64512     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 64)          37248     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 64)          24960     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 38)          2470      \n",
      "=================================================================\n",
      "Total params: 129,190\n",
      "Trainable params: 129,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "      1/Unknown - 0s 0s/step - loss: 3.6366 - accuracy: 0.0217WARNING:tensorflow:From C:\\Users\\souna\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "      2/Unknown - 2s 871ms/step - loss: 3.6297 - accuracy: 0.0544WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4420s vs `on_train_batch_end` time: 1.2999s). Check your callbacks.\n",
      "    663/Unknown - 258s 390ms/step - loss: 2.3048 - accuracy: 0.3329\n",
      "Epoch 00001: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624468336.h5\n",
      "663/663 [==============================] - 259s 390ms/step - loss: 2.3048 - accuracy: 0.3329\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(None, len(word_count))),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(len(word_count), activation='softmax'))\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    dataset, epochs=1, batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c002c81d7d97502821a1873b06c170c28b6572e4320e246f8e08a2c4d790c28"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}