{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file nameshakespeare_sonnets\n",
      "Total character count :  94275\n"
     ]
    }
   ],
   "source": [
    "filename = input('Enter file name')\n",
    "with open(filename+'.txt') as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(\"Total character count : \", len(shakespeare_text))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count :  OrderedDict([('t', 7216), ('h', 5070), ('e', 9249), (' ', 15679), ('s', 4991), ('o', 5707), ('n', 4521), ('\\n', 2468), ('b', 1230), ('y', 1986), ('w', 1898), ('i', 4677), ('l', 3093), ('a', 4941), ('m', 2095), ('k', 553), ('p', 1011), ('r', 4183), ('f', 1663), ('c', 1342), ('u', 2320), ('d', 2763), (',', 1880), (\"'\", 386), ('g', 1358), ('v', 925), (':', 160), ('-', 83), ('.', 396), ('z', 20), (';', 31), ('x', 60), ('?', 92), ('q', 51), ('j', 68), ('(', 43), (')', 43), ('!', 23)]) \n",
      "\n",
      "Document size :  1 \n",
      "\n",
      "Index of tokens {' ': 1, 'e': 2, 't': 3, 'o': 4, 'h': 5, 's': 6, 'a': 7, 'i': 8, 'n': 9, 'r': 10, 'l': 11, 'd': 12, '\\n': 13, 'u': 14, 'm': 15, 'y': 16, 'w': 17, ',': 18, 'f': 19, 'g': 20, 'c': 21, 'b': 22, 'p': 23, 'v': 24, 'k': 25, '.': 26, \"'\": 27, ':': 28, '?': 29, '-': 30, 'j': 31, 'x': 32, 'q': 33, '(': 34, ')': 35, ';': 36, '!': 37, 'z': 38} \n",
      "\n",
      "Unique characters :  38 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count = tokenizer.word_counts\n",
    "document_count = tokenizer.document_count\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Token count : \", word_count, \"\\n\")\n",
    "print(\"Document size : \", document_count, \"\\n\")\n",
    "print(\"Index of tokens\", word_index, \"\\n\")\n",
    "print(\"Unique characters : \", len(word_count), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94275,)\n"
     ]
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded)\n",
    "n_steps = 100\n",
    "window_length = n_steps+1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000001AA87291C10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x000001AA87291C10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = len(word_count)), Y_batch)\n",
    ")\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1624468336']\n",
      "Enter position of model : 0\n",
      "Logging path :  .\\logs_shakespeare_sonnets\\1624468336\\date_2021_06_24-time_11_08_32\n",
      "Loading path :  .\\checkpoints_shakespeare_sonnets\\1624468336\\1624483109.h5\n",
      "Saving path :  .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "checkpoints_filepath = os.path.join(os.curdir, f\"checkpoints_{filename}\")\n",
    "directory = os.listdir(checkpoints_filepath)\n",
    "print(directory)\n",
    "\n",
    "base_model = directory[int(input('Enter position of model : '))]\n",
    "model_name = int(time.time())\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, f\"logs_{filename}\")\n",
    "run_id = time.strftime(\"date_%Y_%m_%d-time_%H_%M_%S\")\n",
    "log_dir = os.path.join(root_logdir, f\"{base_model}\\\\{run_id}\")\n",
    "print(\"Logging path : \", log_dir)\n",
    "\n",
    "root_filepath = os.path.join(os.curdir, f\"checkpoints_{filename}\\\\{base_model}\")\n",
    "file_name = os.listdir(root_filepath)[-1]\n",
    "load_filepath = f\"{root_filepath}\\\\{file_name}\"\n",
    "print(\"Loading path : \", load_filepath)\n",
    "\n",
    "save_filepath = os.path.join(os.curdir, f\"checkpoints_{filename}\\\\{base_model}\\\\{model_name}.h5\")\n",
    "print(\"Saving path : \", save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        save_filepath, monitor='loss',\n",
    "        verbose=1, save_best_only=False\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1,\n",
    "        update_freq=10, write_graph=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, None, 128)         64512     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 64)          37248     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 64)          24960     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 38)          2470      \n",
      "=================================================================\n",
      "Total params: 129,190\n",
      "Trainable params: 129,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Enter the number of epochs : 10\n",
      "Epoch 1/10\n",
      "      1/Unknown - 0s 0s/step - loss: 1.5623 - accuracy: 0.5128WARNING:tensorflow:From C:\\Users\\souna\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "      2/Unknown - 3s 2s/step - loss: 1.5660 - accuracy: 0.5142WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4686s vs `on_train_batch_end` time: 2.9477s). Check your callbacks.\n",
      "    663/Unknown - 253s 382ms/step - loss: 1.4168 - accuracy: 0.5502\n",
      "Epoch 00001: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 254s 382ms/step - loss: 1.4168 - accuracy: 0.5502\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4176 - accuracy: 0.5501\n",
      "Epoch 00002: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 256s 387ms/step - loss: 1.4176 - accuracy: 0.5501\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4167 - accuracy: 0.5504\n",
      "Epoch 00003: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 258s 390ms/step - loss: 1.4167 - accuracy: 0.5504\n",
      "Epoch 4/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4158 - accuracy: 0.5505\n",
      "Epoch 00004: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 265s 399ms/step - loss: 1.4158 - accuracy: 0.5505\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4150 - accuracy: 0.5508\n",
      "Epoch 00005: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 287s 433ms/step - loss: 1.4150 - accuracy: 0.5508\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4146 - accuracy: 0.5509\n",
      "Epoch 00006: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 279s 420ms/step - loss: 1.4146 - accuracy: 0.5509\n",
      "Epoch 7/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4137 - accuracy: 0.5513\n",
      "Epoch 00007: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 275s 414ms/step - loss: 1.4137 - accuracy: 0.5513\n",
      "Epoch 8/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4131 - accuracy: 0.5515\n",
      "Epoch 00008: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 274s 414ms/step - loss: 1.4131 - accuracy: 0.5515\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4129 - accuracy: 0.5515\n",
      "Epoch 00009: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 283s 427ms/step - loss: 1.4129 - accuracy: 0.5515\n",
      "Epoch 10/10\n",
      "663/663 [==============================] - ETA: 0s - loss: 1.4122 - accuracy: 0.5517\n",
      "Epoch 00010: saving model to .\\checkpoints_shakespeare_sonnets\\1624468336\\1624513112.h5\n",
      "663/663 [==============================] - 270s 407ms/step - loss: 1.4122 - accuracy: 0.5517\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(load_filepath)\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "epochs = int(input('Enter the number of epochs : '))\n",
    "history = model.fit(\n",
    "    dataset, epochs=epochs, batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c002c81d7d97502821a1873b06c170c28b6572e4320e246f8e08a2c4d790c28"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
